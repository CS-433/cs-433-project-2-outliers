{
 "cells": [
  {
   "source": [
    "import numpy as np\n",
    "import scipy.io as sio \n",
    "\n",
    "import torch.nn as nn\n",
    "import torch \n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from preprocessing_funcs import get_spikes_with_history, standardize, remove_outliers\n",
    "from model import LSTM\n",
    "from trainer import train\n",
    "from evaluator import test\n",
    "from FingerDataset import FingerDataset\n",
    "from Loss import corr_coeff, corr_coeff_loss"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 13,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_dim, output_dim, batch_size, seq_len, n_hidden= 10 ,n_layers = 1): # no dropout for now \n",
    "        super().__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.seq_len = seq_len\n",
    "        self.batch_size = batch_size\n",
    "        self.n_hidden = n_hidden\n",
    "        self.n_layers = n_layers\n",
    "        \n",
    "       \n",
    "\n",
    "        \"\"\"self.net = nn.Sequential(nn.LSTM(input_dim, hidden_dim, n_layers, batch_first=True), \n",
    "                         nn.LSTM(hidden_dim, hidden_dim, n_layers, batch_first=True), \n",
    "                        nn.Linear(n_hidden, (TrainY.shape[1])))\"\"\"\n",
    "        #lstm layers\n",
    "        self.lstm = nn.LSTM(self.input_dim, self.n_hidden, self.n_layers, batch_first=False)\n",
    "        self.lstm2 = nn.LSTM(self.n_hidden, self.n_hidden, self.n_layers, batch_first=False)\n",
    "        self.lstm3 = nn.LSTM(self.n_hidden, self.n_hidden, self.n_layers, batch_first=False)\n",
    "        #output layer\n",
    "        self.fc = nn.Linear(self.n_hidden, self.output_dim)\n",
    "        self.act = nn.ReLU()\n",
    "    \n",
    "    def binarize_weights(self, ind_layer) : \n",
    "        weights = self.net[ind_layer].weight_ih_l[0] \n",
    "        for w in weights : \n",
    "            if w >= 0 : \n",
    "                w = 1\n",
    "            else : \n",
    "                w = -1 \n",
    "        self.net[ind_layer].weight_ih_l[k]  = weights \n",
    "\n",
    "    \n",
    "    def forward(self, input, hidden):\n",
    "        ''' Forward pass through the network. \n",
    "            These inputs are x, Ifand the hidden/cell state `hidden`. '''\n",
    "        \n",
    "        ## Get the outputs and the new hidden state from the lstm\n",
    "\n",
    "\n",
    "        #our input has shape [batch_size, seq_len, input_dim] but lstm wants [seq_len, batch_size, input_dim]\n",
    "        #reshaping does not ahcieve what we want here so we need to reconstrcut the input the way lstm wants:\n",
    "        new_input = torch.ones((self.seq_len, self.batch_size, self.input_dim))\n",
    "        for i in range(self.seq_len):\n",
    "            new_input[i] = input[:,i,:]\n",
    "       \n",
    "        input = new_input\n",
    "        #input = input.reshape((self.seq_len, self.batch_size, self.input_dim))\n",
    "    \n",
    "        #self.binarize_weights(0)\n",
    "        \n",
    "     \n",
    "        r_output, hidden = self.lstm(new_input, hidden)\n",
    "        #self.binarize_weights(1)\n",
    "        r_output, hidden = self.lstm2(r_output, hidden)\n",
    "\n",
    "        r_output, hidden = self.lstm3(r_output, hidden)\n",
    "        #out = self.act(r_output)\n",
    "        out = self.fc(r_output)\n",
    "        #print(out)\n",
    "       \n",
    "        #print(out)\n",
    "        return out, hidden\n",
    "    \n",
    "    \n",
    "    def init_hidden(self):\n",
    "        ''' Initializes hidden state '''\n",
    "        # Create two new tensors with sizes n_layers x n_hidden,\n",
    "        # initialized to zero, for hidden state and cell state of LSTM\n",
    "        hidden_state = torch.zeros(n_layers, self.batch_size, self.n_hidden)\n",
    "        cell_state = torch.zeros(n_layers, self.batch_size, self.n_hidden)\n",
    "        hidden = (hidden_state, cell_state)\n",
    "\n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_train(batch, net, lossfunc, optimizer, clip = 5):\n",
    "    input, target = batch['input'], batch['target']\n",
    "    \n",
    "    # TODO: Step 1 - create torch variables corresponding to features and labels\n",
    "        \n",
    "\n",
    "    #x = TrainX.reshape([seq_len, TrainX.shape[0],TrainX.shape[1]])\n",
    "    x = input.float()\n",
    "    y = target.float()\n",
    "        \n",
    "    # initialize hidden state \n",
    "    h = net.init_hidden()\n",
    "    # TODO: Step 2 - compute model predictions and loss\n",
    "    pred, h = net(x, h)\n",
    "        \n",
    "    #target = torch.reshape(y, (-1,)).long()\n",
    "   \n",
    "    #loss = lossfunc(pred[-1,:,:], y.squeeze_().long())\n",
    "    \n",
    "    loss = corr_coeff_loss(pred[-1,:,:], y)\n",
    "    # TODO: Step 3 - do a backward pass and a gradient update step\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    # gradient clipping - prevents gradient explosion \n",
    "    nn.utils.clip_grad_norm_(net.parameters(), clip)\n",
    "    optimizer.step()\n",
    "\n",
    "    return pred, target, loss\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(net, dataset, num_epoch=10, batch_size=32):\n",
    "    dataloader = DataLoader(dataset, batch_size, drop_last=True)\n",
    "    lossfunc =  nn.L1Loss()\n",
    "    #optimizer = torch.optim.Adamax(net.parameters(),lr=0.01)\n",
    "    optimizer = torch.optim.SGD(net.parameters(), lr=0.02, momentum=0.9)\n",
    "   \n",
    "    \n",
    "    for epoch in range(num_epoch):\n",
    "        \n",
    "        losses = []\n",
    "        \n",
    "        for batch_idx, batch in enumerate(dataloader):\n",
    "            \n",
    "            pred, y, loss = batch_train(batch, net, lossfunc, optimizer)\n",
    "            losses.append(loss.item())\n",
    "            if batch_idx==0:\n",
    "                preds = pred\n",
    "                ys = y\n",
    "            else:\n",
    "                preds = torch.cat((preds, pred), dim=1)\n",
    "                ys = torch.cat((ys,y), dim=0)\n",
    "            \n",
    "        losses = np.array(losses)\n",
    "       \n",
    "\n",
    "        if (epoch+1)%10 == 0: #num_epoch-1:\n",
    "            #preds = preds[-1,:,:].detach().numpy().reshape((-1,))\n",
    "            #ys = ys.detach().numpy().reshape((-1,))\n",
    "            corrcoef = corr_coeff(preds[-1,:,:],ys).item() #np.corrcoef(preds, ys)\n",
    "            print ('Epoch [%d/%d], Average Batch Loss: %.4f' %(epoch+1, num_epoch, np.mean(losses)))\n",
    "            print ('Correlation Coefficient : {corrcoef}'.format(corrcoef=corrcoef))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(net, dataset, batch_size=32):\n",
    "    dataloader = DataLoader(dataset, batch_size, drop_last=True)\n",
    "\n",
    "    for batch_idx, batch in enumerate(dataloader):\n",
    "        with torch.no_grad():\n",
    "            input, target = batch['input'], batch['target']\n",
    "            h = net.init_hidden()\n",
    "            pred, _ = net(input.float(), h)\n",
    "            target = batch['target']\n",
    "            print(pred)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(9976, 21, 558)\nrun for finger  0\n"
     ]
    }
   ],
   "source": [
    "for Idx_subject in list([10]):#,11,12]): # 3 subjects index 10-12\n",
    "        for Finger in list([0,1,2,3,4]): # 5 fingers for each subject. 0:thumb, 1:index, 2:middle ...\n",
    "\n",
    "            #load training data (TrainX: feature vectors, TrainY: labels)\n",
    "            matData = sio.loadmat('data/BCImoreData_Subj_'+str(Idx_subject)+'_200msLMP.mat')\n",
    "            TrainX = matData['Data_Feature'].transpose()\n",
    "            TrainY = matData['SmoothedFinger']\n",
    "            TrainY = TrainY [:,Finger]\n",
    "            TrainY = TrainY.reshape(TrainY.shape[0],1)\n",
    "            #load testing data (TestX: feature vectors, TestY: labels)\n",
    "            matData = sio.loadmat('data/BCImoreData_Subj_'+str(Idx_subject)+'_200msLMPTest.mat')\n",
    "            TestX = matData['Data_Feature'].transpose()\n",
    "            TestY = matData['SmoothedFinger']\n",
    "            TestY = TestY[:,Finger]\n",
    "            TestY = TestY.reshape(TestY.shape[0],1)\n",
    "            \n",
    "            #standardize and remove outliers from the data\n",
    "            TrainX = standardize(TrainX)\n",
    "            TrainX = remove_outliers(TrainX)\n",
    "            TestX = standardize(TestX)\n",
    "\n",
    "            # from here, we reconstruct the input by \"looking back\" a few steps\n",
    "            bins_before= 20 #How many bins of neural data prior to the output are used for decoding\n",
    "            bins_current=1 #Whether to use concurrent time bin of neural data\n",
    "            bins_after=0 #How many bins of neural data after the output are used for decoding\n",
    "\n",
    "           \n",
    "            \n",
    "            TrainX=get_spikes_with_history(TrainX,bins_before,bins_after,bins_current)\n",
    "\n",
    "            TrainX, TrainY = TrainX[bins_before:,:,:], TrainY[bins_before:,]\n",
    "         \n",
    "            TestX=get_spikes_with_history(TestX,bins_before,bins_after,bins_current)\n",
    "            TestX, TestY = TestX[bins_before:,:,:], TestY[bins_before:,]\n",
    "            \n",
    "            # Now, we reconstructed TrainX/TestX to have a shape (num_of_samples, sequence_length, input_size)\n",
    "            \n",
    "            print(TrainX.shape)\n",
    "            # You can fit this to the LSTM\n",
    "\n",
    "            print(\"run for finger \", Finger)\n",
    "            \n",
    "            input_dim = TrainX.shape[2]\n",
    "            output_dim = TrainY.shape[1]\n",
    "            batch_size = 32\n",
    "            seq_len = TrainX.shape[1]\n",
    "            n_hidden = 50\n",
    "            n_layers = 10\n",
    "\n",
    "            net = LSTM(input_dim, output_dim, batch_size, seq_len, n_hidden, n_layers)\n",
    "            \n",
    "            train_dataset = FingerDataset(TrainX, TrainY)\n",
    "            train(net, train_dataset, num_epoch=100, batch_size=batch_size)\n",
    "            \n",
    "            # Preprocess the data may leed to better performance. e.g. StandardScaler \n",
    "           \n",
    "            #test_dataset = FingerDataset(TestX, TestY)\n",
    "            #test(net, test_dataset, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}