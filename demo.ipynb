{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.io as sio \n",
    "from preprocessing_funcs import get_spikes_with_history\n",
    "from LSTM import LSTM\n",
    "from train import train\n",
    "import torch.nn as nn\n",
    "import torch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    \n",
    "    def __init__(self, TrainX, TrainY, n_hidden= 10 ,n_layers = 1, lr=0.001): # no dropout for now \n",
    "        super().__init__()\n",
    "        self.n_hidden = n_hidden\n",
    "        self.lr = lr\n",
    "        self.n_layers = n_layers\n",
    "        self.input_dim = TrainX.shape[2]\n",
    "        self.output_dim = TrainY.shape[1]\n",
    "        self.seq_len = TrainX.shape[1]\n",
    "        self.batch_size = TrainX.shape[0]\n",
    "\n",
    "        \"\"\"self.net = nn.Sequential(nn.LSTM(input_dim, hidden_dim, n_layers, batch_first=True), \n",
    "                         nn.LSTM(hidden_dim, hidden_dim, n_layers, batch_first=True), \n",
    "                        nn.Linear(n_hidden, (TrainY.shape[1])))\"\"\"\n",
    "        #lstm layers\n",
    "        self.lstm = nn.LSTM(self.input_dim, self.n_hidden, self.n_layers, batch_first=False)\n",
    "        self.lstm2 = nn.LSTM(self.n_hidden, self.n_hidden, self.n_layers, batch_first=False)\n",
    "        #output layer\n",
    "        self.fc = nn.Linear(self.n_hidden, self.output_dim)\n",
    "    \n",
    "    def binarize_weights(self, ind_layer) : \n",
    "        weights = self.net[ind_layer].weight_ih_l[0] \n",
    "        for w in weights : \n",
    "            if w >= 0 : \n",
    "                w = 1\n",
    "            else : \n",
    "                w = -1 \n",
    "        self.net[ind_layer].weight_ih_l[k]  = weights \n",
    "\n",
    "    \n",
    "    def forward(self, TrainX, hidden):\n",
    "        ''' Forward pass through the network. \n",
    "            These inputs are x, and the hidden/cell state `hidden`. '''\n",
    "        ## Get the outputs and the new hidden state from the lstm\n",
    "        TrainX = TrainX.reshape([self.seq_len, self.batch_size, self.input_dim])\n",
    "        #self.binarize_weights(0)\n",
    "        r_output, hidden = self.lstm(TrainX, hidden)\n",
    "        #self.binarize_weights(1)\n",
    "        r_output, hidden = self.lstm2(r_output, hidden)\n",
    "        # Stack up LSTM outputs using view\n",
    "        # reshape the output\n",
    "        #out = r_output.contiguous().view(-1, self.n_hidden)\n",
    "       \n",
    "        ## put x through the fully-connected layer\n",
    "        #self.binarize_weights(2)\n",
    "        out = self.fc(r_output)\n",
    "        return out, hidden\n",
    "    \n",
    "    \n",
    "    def init_hidden(self):\n",
    "        ''' Initializes hidden state '''\n",
    "        # Create two new tensors with sizes n_layers x n_hidden,\n",
    "        # initialized to zero, for hidden state and cell state of LSTM\n",
    "        hidden_state = torch.randn(n_layers, self.batch_size, self.n_hidden)\n",
    "        cell_state = torch.randn(n_layers, self.batch_size, self.n_hidden)\n",
    "        hidden = (hidden_state, cell_state)\n",
    "\n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(TrainX, TrainY, net, lossfunc, optimizer, num_epoch, clip = 5):\n",
    "    seq_len = TrainX.shape[1]\n",
    "\n",
    "    for epoch in range(num_epoch):\n",
    "        # TODO: Step 1 - create torch variables corresponding to features and labels\n",
    "        \n",
    "\n",
    "        #x = TrainX.reshape([seq_len, TrainX.shape[0],TrainX.shape[1]])\n",
    "        x = torch.from_numpy(TrainX).float()\n",
    "        y = torch.from_numpy(TrainY).float()\n",
    "        \n",
    "        # initialize hidden state \n",
    "        h = net.init_hidden()\n",
    "        # TODO: Step 2 - compute model predictions and loss\n",
    "        pred, h = net(x, h)\n",
    "        \n",
    "        #target = torch.reshape(y, (-1,)).long()\n",
    "       \n",
    "        loss = lossfunc(pred, y)\n",
    "        # TODO: Step 3 - do a backward pass and a gradient update step\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        # gradient clipping - prevents gradient explosion \n",
    "        nn.utils.clip_grad_norm_(net.parameters(), clip)\n",
    "        optimizer.step()\n",
    "        \n",
    "        if epoch == num_epoch-1:\n",
    "            print(pred[pred>0])\n",
    "            corrcoef = np.corrcoef(pred[-1,:,:].detach().numpy().reshape((-1,)),y.detach().numpy().reshape((-1,)))\n",
    "            print ('Epoch [%d/%d], Loss: %.4f' %(epoch+1, num_epoch, loss.item()))\n",
    "            print ('Correlation coefficient : {corrcoef}'.format(corrcoef=corrcoef))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "run for finger  0\n",
      "tensor([[[-0.0029],\n",
      "         [-0.0031],\n",
      "         [-0.0023],\n",
      "         ...,\n",
      "         [-0.0022],\n",
      "         [-0.0028],\n",
      "         [-0.0027]],\n",
      "\n",
      "        [[-0.0073],\n",
      "         [-0.0074],\n",
      "         [-0.0067],\n",
      "         ...,\n",
      "         [-0.0068],\n",
      "         [-0.0073],\n",
      "         [-0.0072]],\n",
      "\n",
      "        [[-0.0068],\n",
      "         [-0.0068],\n",
      "         [-0.0062],\n",
      "         ...,\n",
      "         [-0.0064],\n",
      "         [-0.0069],\n",
      "         [-0.0066]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-0.0027],\n",
      "         [-0.0027],\n",
      "         [-0.0027],\n",
      "         ...,\n",
      "         [-0.0027],\n",
      "         [-0.0027],\n",
      "         [-0.0027]],\n",
      "\n",
      "        [[-0.0027],\n",
      "         [-0.0027],\n",
      "         [-0.0027],\n",
      "         ...,\n",
      "         [-0.0027],\n",
      "         [-0.0027],\n",
      "         [-0.0027]],\n",
      "\n",
      "        [[-0.0027],\n",
      "         [-0.0027],\n",
      "         [-0.0027],\n",
      "         ...,\n",
      "         [-0.0027],\n",
      "         [-0.0027],\n",
      "         [-0.0027]]], grad_fn=<AddBackward0>)\n",
      "Epoch [50/50], Loss: 0.2104\n",
      "Correlation coefficient : [[1.      0.01229]\n",
      " [0.01229 1.     ]]\n",
      "run for finger  1\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-6a0af761e615>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     43\u001b[0m             \u001b[0;31m#lossfunc = nn.NLLLoss()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m             \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdamax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m             \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTrainX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTrainY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlossfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclip\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m             \u001b[0;31m# Preprocess the data may leed to better performance. e.g. StandardScaler\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-7fb0c8756cf5>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(TrainX, TrainY, net, lossfunc, optimizer, num_epoch, clip)\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0;31m# TODO: Step 3 - do a backward pass and a gradient update step\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m         \u001b[0;31m# gradient clipping - prevents gradient explosion\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclip\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/EfficientDet/lib/python3.8/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    193\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m         \"\"\"\n\u001b[0;32m--> 195\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    196\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/EfficientDet/lib/python3.8/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     95\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[1;32m     98\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m         allow_unreachable=True)  # allow_unreachable flag\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for Idx_subject in list([10]):#,11,12]): # 3 subjects index 10-12\n",
    "        for Finger in list([0,1,2,3,4]): # 5 fingers for each subject. 0:thumb, 1:index, 2:middle ...\n",
    "\n",
    "            #load training data (TrainX: feature vectors, TrainY: labels)\n",
    "            matData = sio.loadmat('data/BCImoreData_Subj_'+str(Idx_subject)+'_200msLMP.mat')\n",
    "            TrainX = matData['Data_Feature'].transpose()\n",
    "            TrainY = matData['SmoothedFinger']\n",
    "            TrainY = TrainY [:,Finger]\n",
    "            TrainY = TrainY.reshape(TrainY.shape[0],1)\n",
    "            #load testing data (TestX: feature vectors, TestY: labels)\n",
    "            matData = sio.loadmat('data/BCImoreData_Subj_'+str(Idx_subject)+'_200msLMPTest.mat')\n",
    "            TestX = matData['Data_Feature'].transpose()\n",
    "            TestY = matData['SmoothedFinger']\n",
    "            TestY = TestY[:,Finger]\n",
    "            TestY = TestY.reshape(TestY.shape[0],1)\n",
    "            \n",
    "            \n",
    "            \n",
    "            # from here, we reconstruct the input by \"looking back\" a few steps\n",
    "            bins_before= 20 #How many bins of neural data prior to the output are used for decoding\n",
    "            bins_current=1 #Whether to use concurrent time bin of neural data\n",
    "            bins_after=0 #How many bins of neural data after the output are used for decoding\n",
    "            \n",
    "            TrainX=get_spikes_with_history(TrainX,bins_before,bins_after,bins_current)\n",
    "\n",
    "            TrainX, TrainY = TrainX[bins_before:,:,:], TrainY[bins_before:,]\n",
    "         \n",
    "            TestX=get_spikes_with_history(TestX,bins_before,bins_after,bins_current)\n",
    "            TestX, TestY = TestX[bins_before:,:,:], TestY[bins_before:,]\n",
    "            \n",
    "            # Now, we reconstructed TrainX/TestX to have a shape (num_of_samples, sequence_length, input_size)\n",
    "            # You can fit this to the LSTM\n",
    "\n",
    "            print(\"run for finger \", Finger)\n",
    "\n",
    "            n_hidden = 20\n",
    "            n_layers = 5\n",
    "            n_epochs =  50 # start small \n",
    "\n",
    "            net = LSTM(TrainX, TrainY,  n_hidden, n_layers)\n",
    "\n",
    "            lossfunc = nn.L1Loss()\n",
    "            #lossfunc = nn.NLLLoss()\n",
    "            optimizer = torch.optim.Adamax(net.parameters())\n",
    "            train(TrainX, TrainY, net, lossfunc, optimizer, n_epochs, clip = 5)\n",
    "            # Preprocess the data may leed to better performance. e.g. StandardScaler \n",
    "\n",
    "\n",
    "              \n",
    "       \n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}