{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.io as sio \n",
    "from preprocessing_funcs import get_spikes_with_history\n",
    "#from LSTM import LSTM\n",
    "#from train import train\n",
    "import torch.nn as nn\n",
    "import torch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ThresholdPruning(prune.BasePruningMethod):\n",
    "    PRUNING_TYPE = \"unstructured\"\n",
    "\n",
    "    def __init__(self, threshold):\n",
    "        self.threshold = threshold\n",
    "\n",
    "    def compute_mask(self, tensor, default_mask):\n",
    "        return torch.abs(tensor) > self.threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    \n",
    "    def __init__(self, TrainX, TrainY, n_hidden= 10 ,n_layers = 1, lr=0.001): # no dropout for now \n",
    "        super().__init__()\n",
    "        self.n_hidden = n_hidden\n",
    "        self.lr = lr\n",
    "        self.n_layers = n_layers\n",
    "        self.input_dim = TrainX.shape[2]\n",
    "        self.output_dim = TrainY.shape[1]\n",
    "        self.seq_len = TrainX.shape[1]\n",
    "        self.batch_size = TrainX.shape[0]\n",
    "\n",
    "        \"\"\"self.net = nn.Sequential(nn.LSTM(input_dim, hidden_dim, n_layers, batch_first=True), \n",
    "                         nn.LSTM(hidden_dim, hidden_dim, n_layers, batch_first=True), \n",
    "                        nn.Linear(n_hidden, (TrainY.shape[1])))\"\"\"\n",
    "        #lstm layers\n",
    "        self.lstm = nn.LSTM(self.input_dim, self.n_hidden, self.n_layers, batch_first=False)\n",
    "        self.lstm2 = nn.LSTM(self.n_hidden, self.n_hidden, self.n_layers, batch_first=False)\n",
    "        \n",
    "        \n",
    "        #output layer\n",
    "        self.fc = nn.Linear(self.n_hidden, self.output_dim)\n",
    "    \n",
    "    def binarize_weights(self, ind_layer) : \n",
    "        net = self.lstm2\n",
    "        if ind_layer == 0 :\n",
    "            net = self.lstm\n",
    "        weights = net.weight_ih_l0\n",
    "        for idx, w_ in enumerate(weights) : \n",
    "            with torch.no_grad() :\n",
    "                #net.weight_ih_l0[idx] = net.weight_ih_l0[idx].clone()\n",
    "                arr = net.weight_ih_l0[idx].numpy()\n",
    "                arr[arr<0] = -1.\n",
    "                arr[arr>= 0] = 1.\n",
    "\n",
    "    def pruning(self) : \n",
    "        module = net.lstm\n",
    "        module = prune.ln_structured(module, name=\"weight_ih_l0\", n = 1, dim = 0,  amount=0.15)\n",
    "        module = net.lstm2\n",
    "        module = prune.ln_structured(module, name=\"weight_ih_l0\", n=1, dim = 0,  amount=0.15)\n",
    "        #module = net.fc\n",
    "        #module = prune.l1_unstructured(module, name=\"weight\", amount=0.2)\n",
    "    \n",
    "    def threshold_pruning(self) :\n",
    "        parameters_to_prune = ((self.lstm, \"weight_ih_l0\"), (self.lstm2, \"weight_ih_l0\"), (self.fc, \"weight\"))\n",
    "        prune.global_unstructured(parameters_to_prune, pruning_method=ThresholdPruning, threshold= threshold)\n",
    "    \n",
    "    def forward(self, TrainX, hidden):\n",
    "        ''' Forward pass through the network. \n",
    "            These inputs are x, and the hidden/cell state `hidden`. '''\n",
    "        ## Get the outputs and the new hidden state from the lstm\n",
    "        TrainX = TrainX.reshape([self.seq_len, self.batch_size, self.input_dim])\n",
    "        #self.binarize_weights(0)\n",
    "        r_output, hidden = self.lstm(TrainX, hidden)\n",
    "        \n",
    "        # pruning\n",
    "        #self.pruning(1)\n",
    "        \n",
    "        #self.binarize_weights(1)\n",
    "        r_output, hidden = self.lstm2(r_output, hidden)\n",
    "        \n",
    "        # pruning\n",
    "        #self.pruning(2)\n",
    "        \n",
    "        # Stack up LSTM outputs using view\n",
    "        # reshape the output\n",
    "        #out = r_output.contiguous().view(-1, self.n_hidden)\n",
    "       \n",
    "        ## put x through the fully-connected layer\n",
    "        #self.binarize_weights(2)\n",
    "        out = self.fc(r_output)\n",
    "        return out, hidden\n",
    "    \n",
    "    \n",
    "    def init_hidden(self):\n",
    "        ''' Initializes hidden state '''\n",
    "        # Create two new tensors with sizes n_layers x n_hidden,\n",
    "        # initialized to zero, for hidden state and cell state of LSTM\n",
    "        hidden_state = torch.randn(n_layers, self.batch_size, self.n_hidden)\n",
    "        cell_state = torch.randn(n_layers, self.batch_size, self.n_hidden)\n",
    "        hidden = (hidden_state, cell_state)\n",
    "\n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(TrainX, TrainY, net, lossfunc, optimizer, num_epoch, clip = 5, pruning_weights= True, threshold = None):\n",
    "    seq_len = TrainX.shape[1]\n",
    "\n",
    "    for epoch in range(num_epoch):\n",
    "        # TODO: Step 1 - create torch variables corresponding to features and labels\n",
    "        \n",
    "\n",
    "        #x = TrainX.reshape([seq_len, TrainX.shape[0],TrainX.shape[1]])\n",
    "        x = torch.from_numpy(TrainX).float()\n",
    "        y = torch.from_numpy(TrainY).float()\n",
    "        \n",
    "        # initialize hidden state \n",
    "        h = net.init_hidden()\n",
    "        # TODO: Step 2 - compute model predictions and loss\n",
    "        pred, h = net(x, h)\n",
    "        \n",
    "        #target = torch.reshape(y, (-1,)).long()\n",
    "\n",
    "        #loss = lossfunc(pred[-1,:,:], y)\n",
    "        #The main idea is to calculate the correlation as dot product between two features. Larger result indicates more similar\n",
    "        loss = corr_coeff(pred[-1,:,:], y)\n",
    "        \n",
    "        # TODO: Step 3 - do a backward pass and a gradient update step\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        # gradient clipping - prevents gradient explosion \n",
    "        nn.utils.clip_grad_norm_(net.parameters(), clip)\n",
    "        \n",
    "        # pruning after the foward pass \n",
    "        if(pruning_weights) :\n",
    "            if (threshold == None) : \n",
    "            # with pre-implemented prune method \n",
    "                net.pruning()\n",
    "            # with the threshold class \n",
    "            else : \n",
    "                parameters_to_prune = ((net.lstm, \"weight_ih_l0\"), (net.lstm2, \"weight_ih_l0\"))\n",
    "                prune.global_unstructured(parameters_to_prune, pruning_method=ThresholdPruning, threshold= threshold)\n",
    "                print(net.lstm.weight_ih_l0.shape)\n",
    "                print(net.lstm2.weight_ih_l0.shape)\n",
    "                print(net.fc.weight.shape)\n",
    "        optimizer.step()\n",
    "        \n",
    "        \n",
    "        if epoch == num_epoch-1:\n",
    "            print(pred[pred>0])\n",
    "            corrcoef = np.corrcoef(pred[-1,:,:].detach().numpy().reshape((-1,)),y.detach().numpy().reshape((-1,)))\n",
    "            print ('Epoch [%d/%d], Loss: %.4f' %(epoch+1, num_epoch, loss.item()))\n",
    "            print ('Correlation coefficient : {corrcoef}'.format(corrcoef=corrcoef))\n",
    "            \n",
    "            corr_coef.append(np.corrcoef(pred[-1,:,:].detach().numpy().reshape((-1,)),y.detach().numpy().reshape((-1,)))[0,1])\n",
    "            #print('weights', net.lstm.weight_ih_l0)\n",
    "            # count the number of zeros \n",
    "            number_weights_pruned = np.count_nonzero(net.lstm.weight_ih_l0.detach().numpy() ==0)\n",
    "            print('number of pruned lstm: ', number_weights_pruned) \n",
    "            print('total number : ', len(net.lstm.weight_ih_l0.detach().numpy().reshape((-1,))) )\n",
    "            \n",
    "            number_weights_pruned = np.count_nonzero(net.lstm2.weight_ih_l0.detach().numpy() ==0)\n",
    "            print('number of pruned lstm 2: ', number_weights_pruned) \n",
    "            print('total number : ', len(net.lstm2.weight_ih_l0.detach().numpy().reshape((-1,))) )\n",
    "            \n",
    "            number_weights_pruned = np.count_nonzero(net.fc.weight.detach().numpy() ==0)\n",
    "            print('number of pruned fc: ', number_weights_pruned) \n",
    "            print('total number : ', len(net.fc.weight.detach().numpy().reshape((-1,))) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train using correlation coefficient for loss \n",
    "def corr_coeff(x, y):\n",
    "    vx = x - torch.mean(x)\n",
    "    vy = y - torch.mean(y)\n",
    "\n",
    "    cost = torch.mean(vx * vy) / (torch.sqrt(torch.mean(vx ** 2)) * torch.sqrt(torch.mean(vy ** 2)))\n",
    "    return (1/cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "# maybe try robustscaler \n",
    "def preprocessing(X) : \n",
    "    scaler = StandardScaler()\n",
    "    # fit and transform the data \n",
    "    scaled_data = scaler.fit_transform(X)\n",
    "    return scaled_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analytics(analysis) : \n",
    "    nb_pruned = len(net.lstm.weight_ih_l0.detach().numpy().reshape((-1,))) + len(net.lstm2.weight_ih_l0.detach().numpy().reshape((-1,))) + len(net.fc.weight.detach().numpy().reshape((-1,))) \n",
    "    total_weights = 46260\n",
    "    \n",
    "    per_pruned = nb_pruned/total_weights\n",
    "    \n",
    "    analysis.append(per_pruned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "threshold =  0.075\n",
      "SUBJECT :  10\n",
      "preprocessing...\n",
      "run for finger  0\n",
      "Epoch [1/50], Loss: 33.8061\n",
      "Correlation coefficient : [[1.         0.02958043]\n",
      " [0.02958043 1.        ]]\n",
      "Epoch [2/50], Loss: 12.1101\n",
      "Correlation coefficient : [[1.         0.08257581]\n",
      " [0.08257581 1.        ]]\n",
      "Epoch [3/50], Loss: 8.9355\n",
      "Correlation coefficient : [[1.         0.11191343]\n",
      " [0.11191343 1.        ]]\n",
      "Epoch [4/50], Loss: 7.1546\n",
      "Correlation coefficient : [[1.         0.13977033]\n",
      " [0.13977033 1.        ]]\n",
      "Epoch [5/50], Loss: 5.9779\n",
      "Correlation coefficient : [[1.         0.16728323]\n",
      " [0.16728323 1.        ]]\n",
      "Epoch [6/50], Loss: 5.4367\n",
      "Correlation coefficient : [[1.        0.1839343]\n",
      " [0.1839343 1.       ]]\n",
      "Epoch [7/50], Loss: 5.0748\n",
      "Correlation coefficient : [[1.         0.19705372]\n",
      " [0.19705372 1.        ]]\n",
      "Epoch [8/50], Loss: 4.6990\n",
      "Correlation coefficient : [[1.         0.21281151]\n",
      " [0.21281151 1.        ]]\n",
      "Epoch [9/50], Loss: 4.4405\n",
      "Correlation coefficient : [[1.         0.22519895]\n",
      " [0.22519895 1.        ]]\n",
      "Epoch [10/50], Loss: 4.2426\n",
      "Correlation coefficient : [[1.         0.23570402]\n",
      " [0.23570402 1.        ]]\n",
      "Epoch [11/50], Loss: 4.0502\n",
      "Correlation coefficient : [[1.        0.2469031]\n",
      " [0.2469031 1.       ]]\n",
      "Epoch [12/50], Loss: 3.9094\n",
      "Correlation coefficient : [[1.         0.25579511]\n",
      " [0.25579511 1.        ]]\n",
      "Epoch [13/50], Loss: 3.7821\n",
      "Correlation coefficient : [[1.         0.26440096]\n",
      " [0.26440096 1.        ]]\n",
      "Epoch [14/50], Loss: 3.6563\n",
      "Correlation coefficient : [[1.         0.27350432]\n",
      " [0.27350432 1.        ]]\n",
      "Epoch [15/50], Loss: 3.5712\n",
      "Correlation coefficient : [[1.         0.28001526]\n",
      " [0.28001526 1.        ]]\n",
      "Epoch [16/50], Loss: 3.5191\n",
      "Correlation coefficient : [[1.         0.28416742]\n",
      " [0.28416742 1.        ]]\n",
      "Epoch [17/50], Loss: 3.4456\n",
      "Correlation coefficient : [[1.         0.29022424]\n",
      " [0.29022424 1.        ]]\n",
      "Epoch [18/50], Loss: 3.3750\n",
      "Correlation coefficient : [[1.         0.29629731]\n",
      " [0.29629731 1.        ]]\n",
      "Epoch [19/50], Loss: 3.3332\n",
      "Correlation coefficient : [[1.         0.30001572]\n",
      " [0.30001572 1.        ]]\n",
      "Epoch [20/50], Loss: 3.2670\n",
      "Correlation coefficient : [[1.         0.30608748]\n",
      " [0.30608748 1.        ]]\n",
      "Epoch [21/50], Loss: 3.2045\n",
      "Correlation coefficient : [[1.         0.31206486]\n",
      " [0.31206486 1.        ]]\n",
      "Epoch [22/50], Loss: 3.1656\n",
      "Correlation coefficient : [[1.       0.315893]\n",
      " [0.315893 1.      ]]\n",
      "Epoch [23/50], Loss: 3.1198\n",
      "Correlation coefficient : [[1.         0.32052961]\n",
      " [0.32052961 1.        ]]\n",
      "Epoch [24/50], Loss: 3.1277\n",
      "Correlation coefficient : [[1.         0.31972436]\n",
      " [0.31972436 1.        ]]\n",
      "Epoch [25/50], Loss: 3.0618\n",
      "Correlation coefficient : [[1.        0.3266003]\n",
      " [0.3266003 1.       ]]\n",
      "Epoch [26/50], Loss: 3.0269\n",
      "Correlation coefficient : [[1.         0.33037137]\n",
      " [0.33037137 1.        ]]\n",
      "Epoch [27/50], Loss: 3.0158\n",
      "Correlation coefficient : [[1.         0.33158405]\n",
      " [0.33158405 1.        ]]\n",
      "Epoch [28/50], Loss: 2.9958\n",
      "Correlation coefficient : [[1.         0.33379866]\n",
      " [0.33379866 1.        ]]\n",
      "Epoch [29/50], Loss: 2.9327\n",
      "Correlation coefficient : [[1.       0.340983]\n",
      " [0.340983 1.      ]]\n",
      "Epoch [30/50], Loss: 2.9002\n",
      "Correlation coefficient : [[1.        0.3448032]\n",
      " [0.3448032 1.       ]]\n",
      "Epoch [31/50], Loss: 2.8707\n",
      "Correlation coefficient : [[1.         0.34834575]\n",
      " [0.34834575 1.        ]]\n",
      "Epoch [32/50], Loss: 2.8555\n",
      "Correlation coefficient : [[1.         0.35020218]\n",
      " [0.35020218 1.        ]]\n",
      "Epoch [33/50], Loss: 2.8065\n",
      "Correlation coefficient : [[1.         0.35631402]\n",
      " [0.35631402 1.        ]]\n",
      "Epoch [34/50], Loss: 2.7660\n",
      "Correlation coefficient : [[1.         0.36152934]\n",
      " [0.36152934 1.        ]]\n",
      "Epoch [35/50], Loss: 2.7695\n",
      "Correlation coefficient : [[1.         0.36107607]\n",
      " [0.36107607 1.        ]]\n",
      "Epoch [36/50], Loss: 2.7412\n",
      "Correlation coefficient : [[1.         0.36479914]\n",
      " [0.36479914 1.        ]]\n",
      "Epoch [37/50], Loss: 2.7087\n",
      "Correlation coefficient : [[1.         0.36918186]\n",
      " [0.36918186 1.        ]]\n",
      "Epoch [38/50], Loss: 2.6975\n",
      "Correlation coefficient : [[1.         0.37070857]\n",
      " [0.37070857 1.        ]]\n",
      "Epoch [39/50], Loss: 2.6541\n",
      "Correlation coefficient : [[1.         0.37677307]\n",
      " [0.37677307 1.        ]]\n",
      "Epoch [40/50], Loss: 2.6359\n",
      "Correlation coefficient : [[1.         0.37938311]\n",
      " [0.37938311 1.        ]]\n",
      "Epoch [41/50], Loss: 2.6120\n",
      "Correlation coefficient : [[1.         0.38284315]\n",
      " [0.38284315 1.        ]]\n",
      "Epoch [42/50], Loss: 2.5796\n",
      "Correlation coefficient : [[1.         0.38765175]\n",
      " [0.38765175 1.        ]]\n",
      "Epoch [43/50], Loss: 2.5510\n",
      "Correlation coefficient : [[1.         0.39200858]\n",
      " [0.39200858 1.        ]]\n",
      "Epoch [44/50], Loss: 2.5326\n",
      "Correlation coefficient : [[1.         0.39484541]\n",
      " [0.39484541 1.        ]]\n",
      "Epoch [45/50], Loss: 2.5222\n",
      "Correlation coefficient : [[1.         0.39647581]\n",
      " [0.39647581 1.        ]]\n",
      "Epoch [46/50], Loss: 2.4861\n",
      "Correlation coefficient : [[1.         0.40223946]\n",
      " [0.40223946 1.        ]]\n",
      "Epoch [47/50], Loss: 2.4567\n",
      "Correlation coefficient : [[1.         0.40704953]\n",
      " [0.40704953 1.        ]]\n",
      "Epoch [48/50], Loss: 2.4376\n",
      "Correlation coefficient : [[1.         0.41023969]\n",
      " [0.41023969 1.        ]]\n",
      "Epoch [49/50], Loss: 2.4298\n",
      "Correlation coefficient : [[1.         0.41156178]\n",
      " [0.41156178 1.        ]]\n",
      "Epoch [50/50], Loss: 2.4099\n",
      "Correlation coefficient : [[1.         0.41495817]\n",
      " [0.41495817 1.        ]]\n",
      "tensor([], grad_fn=<IndexBackward>)\n",
      "Epoch [50/50], Loss: 2.4099\n",
      "Correlation coefficient : [[1.         0.41495817]\n",
      " [0.41495817 1.        ]]\n",
      "number of pruned lstm:  0\n",
      "total number :  44640\n",
      "number of pruned lstm 2:  0\n",
      "total number :  1600\n",
      "number of pruned fc:  0\n",
      "total number :  20\n",
      "preprocessing...\n",
      "run for finger  1\n",
      "Epoch [1/50], Loss: -110.1132\n",
      "Correlation coefficient : [[ 1.         -0.00908156]\n",
      " [-0.00908156  1.        ]]\n",
      "Epoch [2/50], Loss: 16.9563\n",
      "Correlation coefficient : [[1.         0.05897503]\n",
      " [0.05897503 1.        ]]\n",
      "Epoch [3/50], Loss: 10.9802\n",
      "Correlation coefficient : [[1.         0.09107344]\n",
      " [0.09107344 1.        ]]\n",
      "Epoch [4/50], Loss: 7.6632\n",
      "Correlation coefficient : [[1.         0.13049335]\n",
      " [0.13049335 1.        ]]\n",
      "Epoch [5/50], Loss: 5.9435\n",
      "Correlation coefficient : [[1.         0.16825193]\n",
      " [0.16825193 1.        ]]\n",
      "Epoch [6/50], Loss: 5.1646\n",
      "Correlation coefficient : [[1.         0.19362754]\n",
      " [0.19362754 1.        ]]\n",
      "Epoch [7/50], Loss: 4.5608\n",
      "Correlation coefficient : [[1.         0.21925927]\n",
      " [0.21925927 1.        ]]\n",
      "Epoch [8/50], Loss: 4.2664\n",
      "Correlation coefficient : [[1.         0.23438949]\n",
      " [0.23438949 1.        ]]\n",
      "Epoch [9/50], Loss: 3.9495\n",
      "Correlation coefficient : [[1.         0.25319603]\n",
      " [0.25319603 1.        ]]\n",
      "Epoch [10/50], Loss: 3.6296\n",
      "Correlation coefficient : [[1.         0.27551011]\n",
      " [0.27551011 1.        ]]\n",
      "Epoch [11/50], Loss: 3.4687\n",
      "Correlation coefficient : [[1.         0.28828969]\n",
      " [0.28828969 1.        ]]\n",
      "Epoch [12/50], Loss: 3.3331\n",
      "Correlation coefficient : [[1.         0.30001761]\n",
      " [0.30001761 1.        ]]\n",
      "Epoch [13/50], Loss: 3.2277\n",
      "Correlation coefficient : [[1.        0.3098171]\n",
      " [0.3098171 1.       ]]\n",
      "Epoch [14/50], Loss: 3.1463\n",
      "Correlation coefficient : [[1.         0.31783078]\n",
      " [0.31783078 1.        ]]\n",
      "Epoch [15/50], Loss: 3.0808\n",
      "Correlation coefficient : [[1.         0.32459105]\n",
      " [0.32459105 1.        ]]\n",
      "Epoch [16/50], Loss: 3.0340\n",
      "Correlation coefficient : [[1.         0.32959601]\n",
      " [0.32959601 1.        ]]\n",
      "Epoch [17/50], Loss: 2.9772\n",
      "Correlation coefficient : [[1.        0.3358879]\n",
      " [0.3358879 1.       ]]\n",
      "Epoch [18/50], Loss: 2.9236\n",
      "Correlation coefficient : [[1.        0.3420461]\n",
      " [0.3420461 1.       ]]\n",
      "Epoch [19/50], Loss: 2.8923\n",
      "Correlation coefficient : [[1.         0.34575147]\n",
      " [0.34575147 1.        ]]\n",
      "Epoch [20/50], Loss: 2.8241\n",
      "Correlation coefficient : [[1.         0.35409945]\n",
      " [0.35409945 1.        ]]\n",
      "Epoch [21/50], Loss: 2.7827\n",
      "Correlation coefficient : [[1.        0.3593682]\n",
      " [0.3593682 1.       ]]\n",
      "Epoch [22/50], Loss: 2.7564\n",
      "Correlation coefficient : [[1.         0.36278622]\n",
      " [0.36278622 1.        ]]\n",
      "Epoch [23/50], Loss: 2.7123\n",
      "Correlation coefficient : [[1.        0.3686844]\n",
      " [0.3686844 1.       ]]\n",
      "Epoch [24/50], Loss: 2.7012\n",
      "Correlation coefficient : [[1.         0.37021236]\n",
      " [0.37021236 1.        ]]\n",
      "Epoch [25/50], Loss: 2.6574\n",
      "Correlation coefficient : [[1.         0.37630658]\n",
      " [0.37630658 1.        ]]\n",
      "Epoch [26/50], Loss: 2.6435\n",
      "Correlation coefficient : [[1.         0.37829335]\n",
      " [0.37829335 1.        ]]\n",
      "Epoch [27/50], Loss: 2.6150\n",
      "Correlation coefficient : [[1.         0.38241042]\n",
      " [0.38241042 1.        ]]\n",
      "Epoch [28/50], Loss: 2.6064\n",
      "Correlation coefficient : [[1.         0.38366528]\n",
      " [0.38366528 1.        ]]\n",
      "Epoch [29/50], Loss: 2.5627\n",
      "Correlation coefficient : [[1.         0.39020731]\n",
      " [0.39020731 1.        ]]\n",
      "Epoch [30/50], Loss: 2.5588\n",
      "Correlation coefficient : [[1.         0.39080205]\n",
      " [0.39080205 1.        ]]\n",
      "Epoch [31/50], Loss: 2.5379\n",
      "Correlation coefficient : [[1.        0.3940327]\n",
      " [0.3940327 1.       ]]\n",
      "Epoch [32/50], Loss: 2.5221\n",
      "Correlation coefficient : [[1.         0.39650082]\n",
      " [0.39650082 1.        ]]\n",
      "Epoch [33/50], Loss: 2.5015\n",
      "Correlation coefficient : [[1.         0.39975629]\n",
      " [0.39975629 1.        ]]\n",
      "Epoch [34/50], Loss: 2.4745\n",
      "Correlation coefficient : [[1.         0.40412223]\n",
      " [0.40412223 1.        ]]\n",
      "Epoch [35/50], Loss: 2.4611\n",
      "Correlation coefficient : [[1.         0.40632843]\n",
      " [0.40632843 1.        ]]\n",
      "Epoch [36/50], Loss: 2.4436\n",
      "Correlation coefficient : [[1.         0.40924008]\n",
      " [0.40924008 1.        ]]\n",
      "Epoch [37/50], Loss: 2.4303\n",
      "Correlation coefficient : [[1.         0.41146533]\n",
      " [0.41146533 1.        ]]\n",
      "Epoch [38/50], Loss: 2.4115\n",
      "Correlation coefficient : [[1.         0.41467666]\n",
      " [0.41467666 1.        ]]\n",
      "Epoch [39/50], Loss: 2.3998\n",
      "Correlation coefficient : [[1.         0.41670654]\n",
      " [0.41670654 1.        ]]\n",
      "Epoch [40/50], Loss: 2.3832\n",
      "Correlation coefficient : [[1.         0.41961215]\n",
      " [0.41961215 1.        ]]\n",
      "Epoch [41/50], Loss: 2.3761\n",
      "Correlation coefficient : [[1.         0.42085552]\n",
      " [0.42085552 1.        ]]\n",
      "Epoch [42/50], Loss: 2.3574\n",
      "Correlation coefficient : [[1.         0.42420087]\n",
      " [0.42420087 1.        ]]\n",
      "Epoch [43/50], Loss: 2.3485\n",
      "Correlation coefficient : [[1.         0.42580736]\n",
      " [0.42580736 1.        ]]\n",
      "Epoch [44/50], Loss: 2.3282\n",
      "Correlation coefficient : [[1.         0.42951189]\n",
      " [0.42951189 1.        ]]\n",
      "Epoch [45/50], Loss: 2.3182\n",
      "Correlation coefficient : [[1.         0.43136974]\n",
      " [0.43136974 1.        ]]\n",
      "Epoch [46/50], Loss: 2.3096\n",
      "Correlation coefficient : [[1.         0.43297072]\n",
      " [0.43297072 1.        ]]\n",
      "Epoch [47/50], Loss: 2.2851\n",
      "Correlation coefficient : [[1.        0.4376192]\n",
      " [0.4376192 1.       ]]\n",
      "Epoch [48/50], Loss: 2.2798\n",
      "Correlation coefficient : [[1.         0.43864344]\n",
      " [0.43864344 1.        ]]\n",
      "Epoch [49/50], Loss: 2.2692\n",
      "Correlation coefficient : [[1.         0.44068673]\n",
      " [0.44068673 1.        ]]\n",
      "Epoch [50/50], Loss: 2.2502\n",
      "Correlation coefficient : [[1.         0.44441372]\n",
      " [0.44441372 1.        ]]\n",
      "tensor([], grad_fn=<IndexBackward>)\n",
      "Epoch [50/50], Loss: 2.2502\n",
      "Correlation coefficient : [[1.         0.44441372]\n",
      " [0.44441372 1.        ]]\n",
      "number of pruned lstm:  0\n",
      "total number :  44640\n",
      "number of pruned lstm 2:  0\n",
      "total number :  1600\n",
      "number of pruned fc:  0\n",
      "total number :  20\n",
      "preprocessing...\n",
      "run for finger  2\n",
      "Epoch [1/50], Loss: 196.6499\n",
      "Correlation coefficient : [[1.         0.00508518]\n",
      " [0.00508518 1.        ]]\n",
      "Epoch [2/50], Loss: 15.0664\n",
      "Correlation coefficient : [[1.        0.0663727]\n",
      " [0.0663727 1.       ]]\n",
      "Epoch [3/50], Loss: 9.9590\n",
      "Correlation coefficient : [[1.         0.10041188]\n",
      " [0.10041188 1.        ]]\n",
      "Epoch [4/50], Loss: 7.4323\n",
      "Correlation coefficient : [[1.         0.13454815]\n",
      " [0.13454815 1.        ]]\n",
      "Epoch [5/50], Loss: 6.2820\n",
      "Correlation coefficient : [[1.         0.15918559]\n",
      " [0.15918559 1.        ]]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-242-0ada6b493381>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     61\u001b[0m             \u001b[1;31m#lossfunc = nn.NLLLoss()\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m             \u001b[0moptimizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mAdamax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m             \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mTrainX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mTrainY\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnet\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlossfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_epochs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclip\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpruning_weights\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m             \u001b[1;31m#net.threshold_pruning()\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-238-d64ed877716a>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(TrainX, TrainY, net, lossfunc, optimizer, num_epoch, clip, pruning_weights, threshold)\u001b[0m\n\u001b[0;32m     23\u001b[0m         \u001b[1;31m# TODO: Step 3 - do a backward pass and a gradient update step\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m         \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     26\u001b[0m         \u001b[1;31m# gradient clipping - prevents gradient explosion\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m         \u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclip\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[0;32m    219\u001b[0m                 \u001b[0mretain_graph\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    220\u001b[0m                 create_graph=create_graph)\n\u001b[1;32m--> 221\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    222\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    223\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[0;32m    130\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[0;32m    131\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 132\u001b[1;33m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[0;32m    133\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    134\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#list_ = [0.01, 0.05, 0.1, 0.125, 0.15, 0.175]\n",
    "#list_ = [0, 0.06, 0.07, 0.075, 0.08]\n",
    "list_ = [0.075]\n",
    "#should contained percentage of pruned for each threshold \n",
    "analysis = []\n",
    "corr_coef = []\n",
    "\n",
    "for threshold in list_ : \n",
    "    print('threshold = ', threshold)\n",
    "    for Idx_subject in list([10,11,12]): # 3 subjects index 10-12\n",
    "        print(\"SUBJECT : \", Idx_subject)\n",
    "        for Finger in list([0,1,2,3,4]): # 5 fingers for each subject. 0:thumb, 1:index, 2:middle ...\n",
    "\n",
    "            #load training data (TrainX: feature vectors, TrainY: labels)\n",
    "            matData = sio.loadmat('data/BCImoreData_Subj_'+str(Idx_subject)+'_200msLMP.mat')\n",
    "            TrainX = matData['Data_Feature'].transpose()\n",
    "            TrainY = matData['SmoothedFinger']\n",
    "            TrainY = TrainY [:,Finger]\n",
    "            TrainY = TrainY.reshape(TrainY.shape[0],1)\n",
    "            #load testing data (TestX: feature vectors, TestY: labels)\n",
    "            matData = sio.loadmat('data/BCImoreData_Subj_'+str(Idx_subject)+'_200msLMPTest.mat')\n",
    "            TestX = matData['Data_Feature'].transpose()\n",
    "            TestY = matData['SmoothedFinger']\n",
    "            TestY = TestY[:,Finger]\n",
    "            TestY = TestY.reshape(TestY.shape[0],1)\n",
    "\n",
    "            # preprocessing \n",
    "            print(\"preprocessing...\")\n",
    "            TrainX = preprocessing(TrainX)\n",
    "            TestX = preprocessing(TestX)\n",
    "\n",
    "            # from here, we reconstruct the input by \"looking back\" a few steps\n",
    "            bins_before= 20 #How many bins of neural data prior to the output are used for decoding\n",
    "            bins_current=1 #Whether to use concurrent time bin of neural data\n",
    "            bins_after=0 #How many bins of neural data after the output are used for decoding\n",
    "\n",
    "            TrainX=get_spikes_with_history(TrainX,bins_before,bins_after,bins_current)\n",
    "\n",
    "            TrainX, TrainY = TrainX[bins_before:,:,:], TrainY[bins_before:,]\n",
    "\n",
    "            TestX=get_spikes_with_history(TestX,bins_before,bins_after,bins_current)\n",
    "            TestX, TestY = TestX[bins_before:,:,:], TestY[bins_before:,]\n",
    "\n",
    "            # Now, we reconstructed TrainX/TestX to have a shape (num_of_samples, sequence_length, input_size)\n",
    "            # You can fit this to the LSTM\n",
    "\n",
    "\n",
    "            print(\"run for finger \", Finger)\n",
    "\n",
    "            n_hidden = 20\n",
    "            n_layers = 5\n",
    "\n",
    "\n",
    "            n_epochs = 50 # start small \n",
    "\n",
    "            net = LSTM(TrainX, TrainY,  n_hidden, n_layers)\n",
    "\n",
    "            lossfunc = nn.L1Loss()\n",
    "\n",
    "\n",
    "            optimizer = torch.optim.Adamax(net.parameters())\n",
    "            train(TrainX, TrainY, net, lossfunc, optimizer, n_epochs, clip = 5, pruning_weights = False)\n",
    "\n",
    "            net.threshold_pruning()\n",
    "            \n",
    "            # store them in a sparse tensor \n",
    "            #net.lstm.weight_ih_l0 = torch.nn.Parameter(net.lstm.weight_ih_l0.data.to_sparse())\n",
    "            #net.lstm2.weight_ih_l0 = torch.nn.Parameter(net.lstm2.weight_ih_l0.data.to_sparse()) \n",
    "                \n",
    "                \n",
    "            #analytics(analysis)\n",
    "\n",
    "            # retrain it the pruned network to improve accuracy  \n",
    "            print(\"retrain the pruned network \")\n",
    "            n_epochs =  50\n",
    "\n",
    "            train(TrainX, TrainY, net, lossfunc, optimizer, n_epochs, clip = 5, pruning_weights = False)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(analysis)\n",
    "#analysis_stored = analysis\n",
    "len(corr_coef)\n",
    "#corr_coef_stored = corr_coef "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pruning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.utils.prune as prune\n",
    "\n",
    "# select a pruning technique from pytorch \n",
    "\n",
    "# choose the percentage of connections hat you would like to prune \n",
    "\n",
    "# it has do be pruned and then retrained on the remaining weights so the accuracy can go up again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pruning does work, more and more weight units are set to zero while running. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# todo : \n",
    "- compute the number of pruned connection for each layer and globally \n",
    "- try with pruning on the linear layer \n",
    "- understand how to store the sparse weights tensor (ask Bingzhao about the paper)\n",
    "- find the right lethod of pruning \n",
    "- see if we can adapt it to the finger number "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# done : \n",
    "- try with multiple layers \n",
    "- find the right percentage of connections to prune \n",
    "- see if there is a dimension (channel) that is better for pruning than another \n",
    "- try to do our own class with a threshold instead of a percentage of "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "re run with printing the number of weights pruned for the lstm2 !!!!!!!!!!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
