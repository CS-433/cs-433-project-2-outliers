{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.io as sio \n",
    "from preprocessing_funcs import get_spikes_with_history\n",
    "import torch.nn as nn\n",
    "import torch \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "verbose = False #set this if you want to print the corr coef at each epoch\n",
    "\n",
    "import torch.nn.utils.prune as prune\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mount with the drive\n",
    "import os\n",
    "\n",
    "# # need to run only first time\n",
    "main_path = os.path.join('data')\n",
    "result_path = os.path.join('results', 'ML_DATA')\n",
    "figure_path = os.path.join('results', 'ML_DATA')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ThresholdPruning(prune.BasePruningMethod):\n",
    "    PRUNING_TYPE = \"unstructured\"\n",
    "\n",
    "    def __init__(self, threshold):\n",
    "        self.threshold = threshold\n",
    "\n",
    "    def compute_mask(self, tensor, default_mask):\n",
    "        return torch.abs(tensor) > self.threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    \n",
    "    def __init__(self, TrainX, TrainY, n_hidden= 5 ,n_layers = 1, lr=0.001): # no dropout for now \n",
    "        super().__init__()\n",
    "        self.n_hidden = n_hidden\n",
    "        self.lr = lr\n",
    "        self.n_layers = n_layers\n",
    "        self.input_dim = TrainX.shape[2]\n",
    "        self.output_dim = TrainY.shape[1]\n",
    "        self.seq_len = TrainX.shape[1]\n",
    "        self.batch_size = TrainX.shape[0]\n",
    "\n",
    "        \"\"\"self.net = nn.Sequential(nn.LSTM(input_dim, hidden_dim, n_layers, batch_first=True), \n",
    "                         nn.LSTM(hidden_dim, hidden_dim, n_layers, batch_first=True), \n",
    "                        nn.Linear(n_hidden, (TrainY.shape[1])))\"\"\"\n",
    "        # lstm layers\n",
    "        self.lstm = nn.LSTM(self.input_dim, self.n_hidden, self.n_layers, batch_first=False)\n",
    "        self.lstm2 = nn.LSTM(self.n_hidden, self.n_hidden, self.n_layers, batch_first=False)\n",
    "        #output layer\n",
    "        self.fc = nn.Linear(self.n_hidden, self.output_dim)\n",
    "        \n",
    "    def binarize_weights(self, ind_layer) : \n",
    "        net = self.lstm2\n",
    "        if ind_layer == 0 :\n",
    "            net = self.lstm\n",
    "        weights = net.weight_ih_l0\n",
    "        for idx, w_ in enumerate(weights) : \n",
    "            with torch.no_grad() :\n",
    "                #net.weight_ih_l0[idx] = net.weight_ih_l0[idx].clone()\n",
    "                arr = net.weight_ih_l0[idx].numpy()\n",
    "                arr[arr<0] = -1.\n",
    "                arr[arr>= 0] = 1.\n",
    "\n",
    "    \n",
    "    def threshold_pruning(self) :\n",
    "        parameters_to_prune = ((self.lstm, \"weight_ih_l0\"), (self.lstm2, \"weight_ih_l0\"), (self.fc, \"weight\"))\n",
    "        prune.global_unstructured(parameters_to_prune, pruning_method=ThresholdPruning, threshold= 0.075)\n",
    "        print(\"pruning\")\n",
    "\n",
    "    def forward(self, TrainX, hidden):\n",
    "        ''' Forward pass through the network. \n",
    "            These inputs are x, and the hidden/cell state `hidden`. '''\n",
    "        ## Get the outputs and the new hidden state from the lstm\n",
    "        new_input = torch.ones((self.seq_len, TrainX.shape[0], self.input_dim))\n",
    "        for i in range(self.seq_len):\n",
    "            new_input[i] = TrainX[:,i,:]\n",
    "       \n",
    "        TrainX = new_input\n",
    "        r_output, hidden = self.lstm(TrainX, hidden)\n",
    "        r_output, hidden = self.lstm2(r_output, hidden)\n",
    "       \n",
    "        ## put x through the fully-connected layer\n",
    "        out = self.fc(r_output)\n",
    "        return out, hidden\n",
    "    \n",
    "    \n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        ''' Initializes hidden state '''\n",
    "        # Create two new tensors with sizes n_layers x n_hidden,\n",
    "        # initialized to zero, for hidden state and cell state of LSTM\n",
    "        hidden_state = torch.zeros(n_layers, batch_size, self.n_hidden)\n",
    "        cell_state = torch.randn(n_layers, batch_size, self.n_hidden)\n",
    "        hidden = (hidden_state, cell_state)\n",
    "\n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "\n",
    "def train(TrainX, TrainY,TestX, TestY, net, lossfunc, optimizer, num_epoch, clip = 5, finger =0):\n",
    "    seq_len = TrainX.shape[1]\n",
    "    train_length = int(0.8*len(TrainX))\n",
    "    val_length = len(TrainX)-train_length\n",
    "    train_data = TrainX[:train_length]\n",
    "    val_data = TrainX[train_length:]\n",
    "    train_label = TrainY[:train_length]\n",
    "    val_label = TrainY[train_length:]\n",
    "    list_corr_train = []\n",
    "    list_corr_val = []\n",
    "    list_corr_test = []\n",
    "    \n",
    "   \n",
    "    for epoch in range(num_epoch):\n",
    "        # TODO: Step 1 - create torch variables corresponding to features and labels\n",
    "\n",
    "        #x = TrainX.reshape([seq_len, TrainX.shape[0],TrainX.shape[1]])\n",
    "        x = torch.from_numpy(train_data).float()\n",
    "        y = torch.from_numpy(train_label).float()\n",
    "        xv = torch.from_numpy(val_data).float()\n",
    "        yv = torch.from_numpy(val_label).float()\n",
    "        xt = torch.from_numpy(TestX).float()\n",
    "        yt = torch.from_numpy(TestY).float()\n",
    "        # initialize hidden state \n",
    "        h = net.init_hidden(train_data.shape[0])\n",
    "        hv = net.init_hidden(val_data.shape[0])\n",
    "        ht = net.init_hidden(TestX.shape[0])\n",
    "        # TODO: Step 2 - compute model predictions and loss\n",
    "        pred, h = net(x, h)\n",
    "        \n",
    "        \n",
    "        loss = lossfunc(pred[-1,:,:], y)\n",
    "        #The main idea is to calculate the correlation as dot product between two features. Larger result indicates more similar\n",
    "        #loss = corr_coeff(pred[-1,:,:], y)\n",
    "        \n",
    "        # TODO: Step 3 - do a backward pass and a gradient update step\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        # gradient clipping - prevents gradient explosion \n",
    "        nn.utils.clip_grad_norm_(net.parameters(), clip)\n",
    "        optimizer.step()\n",
    "        corrcoef_train = np.corrcoef(pred[-1,:,:].detach().numpy().reshape((-1,)),y.detach().numpy().reshape((-1,)))\n",
    "        list_corr_train += [corrcoef_train[0,1]]\n",
    "        predv, hv = net(xv, hv)\n",
    "        corrcoefv = np.corrcoef(predv[-1,:,:].detach().numpy().reshape((-1,)),yv.detach().numpy().reshape((-1,)))\n",
    "        list_corr_val += [corrcoefv[0,1]]\n",
    "        predt, ht = net(xt, ht)\n",
    "        corrcoeft = np.corrcoef(predt[-1,:,:].detach().numpy().reshape((-1,)),yt.detach().numpy().reshape((-1,)))\n",
    "        list_corr_test += [corrcoeft[0,1]]\n",
    "        \n",
    "        if verbose:\n",
    "            print ('Epoch [%d/%d], Loss: %.4f' %(epoch+1, num_epoch, loss.item()))\n",
    "            print ('Correlation coefficient train : {corrcoef}'.format(corrcoef=corrcoef_train[0,1]))\n",
    "            print ('Correlation coefficient validation: {corrcoef}'.format(corrcoef=corrcoefv[0,1]))\n",
    "            print ('Correlation coefficient test: {corrcoef}'.format(corrcoef=corrcoeft[0,1]))\n",
    "    return list_corr_train, list_corr_val, list_corr_test\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train using correlation coefficient for loss \n",
    "def corr_coeff(x, y):\n",
    "    vx = x - torch.mean(x)\n",
    "    vy = y - torch.mean(y)\n",
    "\n",
    "    cost = torch.mean(vx * vy) / (torch.sqrt(torch.mean(vx ** 2)) * torch.sqrt(torch.mean(vy ** 2)))\n",
    "    return (1 - cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing \n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "def preprocessing(X, testX, Y, testY) : \n",
    "    \n",
    "    # fit and transform the data \n",
    "    scaler_data = scaler.fit_transform(X)\n",
    "    testX = scaler.transform(testX)\n",
    "    return scaler_data, testX, Y, testY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preprocessing...\n",
      "run for finger  0\n",
      "training \n"
     ]
    }
   ],
   "source": [
    "#list_ = [0.01, 0.05, 0.1, 0.125, 0.15, 0.175]\n",
    "#list_ = [0.06, 0.07, 0.075, 0.08]\n",
    "list_ = [0.075]\n",
    "\n",
    "import pickle\n",
    "\n",
    "   \n",
    "\"for threshold in list_ : \"\n",
    "    \n",
    "for Idx_subject in list([10]): # 3 subjects index 10-12\n",
    "        prediction = [[],[],[],[],[],[]]\n",
    "        predictionTest = [[],[],[],[],[],[]]\n",
    "        model = [[],[],[],[],[],[]]\n",
    "        all_corrcoef = [[],[],[],[],[],[]]\n",
    "        all_corrcoef_test = [[],[],[],[],[],[]]\n",
    "        all_corrcoef_val = [[],[],[],[],[],[]]\n",
    "        for Finger in list([0, 1, 2, 3, 4]): # 5 fingers for each subject. 0:thumb, 1:index, 2:middle ...\n",
    "            \n",
    "            #load training data (TrainX: feature vectors, TrainY: labels)\n",
    "            matData = sio.loadmat(main_path + '/BCImoreData_Subj_'+str(Idx_subject)+'_200msLMP.mat')\n",
    "            TrainX = matData['Data_Feature'].transpose()\n",
    "            TrainY = matData['SmoothedFinger']\n",
    "            TrainY = TrainY [:,Finger]\n",
    "            TrainY = TrainY.reshape(TrainY.shape[0],1)\n",
    "            #load testing data (TestX: feature vectors, TestY: labels)\n",
    "            matData = sio.loadmat(main_path + '/BCImoreData_Subj_'+str(Idx_subject)+'_200msLMPTest.mat')\n",
    "            TestX = matData['Data_Feature'].transpose()\n",
    "            TestY = matData['SmoothedFinger']\n",
    "            TestY = TestY[:,Finger]\n",
    "            TestY = TestY.reshape(TestY.shape[0],1)\n",
    "            \n",
    "            # preprocessing \n",
    "            print(\"preprocessing...\")\n",
    "            TrainX,TestX,TrainY,TestY  = preprocessing(TrainX,TestX,TrainY,TestY)\n",
    "            \n",
    "\n",
    "            # from here, we reconstruct the input by \"looking back\" a few steps\n",
    "            bins_before= 20 #How many bins of neural data prior to the output are used for decoding\n",
    "            bins_current=1 #Whether to use concurrent time bin of neural data\n",
    "            bins_after=0 #How many bins of neural data after the output are used for decoding\n",
    "            \n",
    "            TrainX=get_spikes_with_history(TrainX,bins_before,bins_after,bins_current)\n",
    "\n",
    "            TrainX, TrainY = TrainX[bins_before:,:,:], TrainY[bins_before:,]\n",
    "         \n",
    "            TestX=get_spikes_with_history(TestX,bins_before,bins_after,bins_current)\n",
    "            TestX, TestY = TestX[bins_before:,:,:], TestY[bins_before:,]\n",
    "            \n",
    "            # Now, we reconstructed TrainX/TestX to have a shape (num_of_samples, sequence_length, input_size)\n",
    "            # You can fit this to the LSTM\n",
    "            \n",
    "            print(\"run for finger \", Finger)\n",
    "\n",
    "            n_hidden = 20\n",
    "            n_layers = 5\n",
    "            n_epochs =  50 #CHANGE THIS\n",
    "\n",
    "            net = LSTM(TrainX, TrainY,  n_hidden, n_layers)\n",
    "            \n",
    "            lossfunc = nn.MSELoss()\n",
    "            optimizer = torch.optim.Adamax(net.parameters())\n",
    "            \n",
    "            print(\"training \")\n",
    "            net.train()\n",
    "  \n",
    "            n_epochs =  50\n",
    "            net.train()\n",
    "\n",
    "            corr_train, corr_val, corr_test = train(TrainX, TrainY,TestX,TestY, net, lossfunc, optimizer,n_epochs, clip = 5, finger = Finger)\n",
    "            net.eval()\n",
    "            trainPred,h = net(torch.from_numpy(TrainX).float(), net.init_hidden(TrainX.shape[0]))\n",
    "            prediction[Finger] = trainPred[-1,:,:].detach().numpy().reshape((-1,))\n",
    "            corrcoef = np.corrcoef(prediction[Finger],TrainY.reshape((-1,)))\n",
    "            all_corrcoef[Finger] = corr_train\n",
    "            print ('Correlation coefficient train : {corrcoef}'.format(corrcoef=corrcoef[0,1]))\n",
    "            model[Finger] = net\n",
    "            testPred,h = net(torch.from_numpy(TestX).float(), net.init_hidden(TestX.shape[0]))\n",
    "            predictionTest[Finger] = testPred[-1,:,:].detach().numpy().reshape((-1,))\n",
    "            corrcoef = np.corrcoef(predictionTest[Finger],TestY.reshape((-1,)))\n",
    "            all_corrcoef_test[Finger] = corr_test\n",
    "            all_corrcoef_val[Finger] = corr_val\n",
    "            print ('Correlation coefficient test : {corrcoef}'.format(corrcoef=corrcoef[0,1]))  \n",
    "            \n",
    "            \n",
    "            net.threshold_pruning()\n",
    "\n",
    "            print(\"retrain the pruned network \")\n",
    "            n_epochs =  100\n",
    "            corr_train, corr_val, corr_test = train(TrainX, TrainY,TestX,TestY, net, lossfunc, optimizer,n_epochs, clip = 5, finger = Finger)\n",
    "            net.eval()\n",
    "            trainPred,h = net(torch.from_numpy(TrainX).float(), net.init_hidden(TrainX.shape[0]))\n",
    "            prediction[Finger] = trainPred[-1,:,:].detach().numpy().reshape((-1,))\n",
    "            corrcoef = np.corrcoef(prediction[Finger],TrainY.reshape((-1,)))\n",
    "            all_corrcoef[Finger] = corr_train\n",
    "            print ('Correlation coefficient train : {corrcoef}'.format(corrcoef=corrcoef[0,1]))\n",
    "            model[Finger] = net\n",
    "            testPred,h = net(torch.from_numpy(TestX).float(), net.init_hidden(TestX.shape[0]))\n",
    "            predictionTest[Finger] = testPred[-1,:,:].detach().numpy().reshape((-1,))\n",
    "            corrcoef = np.corrcoef(predictionTest[Finger],TestY.reshape((-1,)))\n",
    "            all_corrcoef_test[Finger] = corr_test\n",
    "            all_corrcoef_val[Finger] = corr_val\n",
    "            print ('Correlation coefficient test : {corrcoef}'.format(corrcoef=corrcoef[0,1])) \n",
    "            \n",
    "        #with open(os.path.join(result_path, 'corrcoefNoBin_'+str(n_epochs)+'E_Sbj' + str(Idx_subject)), 'wb') as f:\n",
    "        #  pickle.dump(all_corrcoef, f) \n",
    "        #with open(os.path.join(result_path, 'corrcoefNoBin_val_'+str(n_epochs)+'E_Sbj' + str(Idx_subject)), 'wb') as f:\n",
    "        #  pickle.dump(all_corrcoef_val, f)    \n",
    "        #with open(os.path.join(result_path, 'corrcoefNoBin__test_'+str(n_epochs)+'E_Sbj' + str(Idx_subject)), 'wb') as f:\n",
    "        #  pickle.dump(all_corrcoef_test, f)  \n",
    "        with open(os.path.join(result_path, 'finalPredictionsNoBin_'+str(n_epochs)+'E_Sbj' + str(Idx_subject)), 'wb') as f:\n",
    "            pickle.dump(prediction, f)\n",
    "        with open(os.path.join(result_path, 'finalmodelNoBin_'+str(n_epochs)+'E_Sbj' + str(Idx_subject)), 'wb') as fi:\n",
    "            pickle.dump(model, fi)\n",
    "        with open(os.path.join(result_path, 'finaltestPredictionsNoBin_'+str(n_epochs)+'E_Sbj' + str(Idx_subject)), 'wb') as f:\n",
    "            pickle.dump(predictionTest, f)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "res_100 = pd.read_pickle('results/ML_DATA/TestpredictionsPruning_200E_Sbj10')\n",
    "res_100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(result_path, 'corrcoef_testNoBin_100E_Sbj10'), 'rb') as f:\n",
    "    all_corrcoef_test = pickle.load(f)\n",
    "with open(os.path.join(result_path, 'corrcoef_valNoBin_100E_Sbj10'), 'rb') as f:\n",
    "    all_corrcoef_val = pickle.load(f)\n",
    "with open(os.path.join(result_path, 'corrcoefNoBin_100E_Sbj10'), 'rb') as f:\n",
    "    all_corrcoef = pickle.load(f)\n",
    "fig_corrcoef = plt.figure(figsize=(15,10))\n",
    "plt.plot( all_corrcoef[0])\n",
    "plt.plot( all_corrcoef_val[0])\n",
    "plt.plot( all_corrcoef_test[0])\n",
    "plt.title('Correlation coefficient evaluation for 100 epoch without binarization ')\n",
    "\n",
    "plt.legend([\"Train corrcoef \", \"Validation corrcoef \", \"Test corrcoef\"], loc = 'best')\n",
    "fig_corrcoef.savefig(os.path.join(figure_path, 'Corrcoef '+str(len(all_corrcoef_test))+' epochs with binarization subj 10 finger 0.png'), format='png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(result_path, 'corrcoef_testBin_100E_Sbj10'), 'rb') as f:\n",
    "    all_corrcoef_test = pickle.load(f)\n",
    "with open(os.path.join(result_path, 'corrcoef_valBin_100E_Sbj10'), 'rb') as f:\n",
    "    all_corrcoef_val = pickle.load(f)\n",
    "with open(os.path.join(result_path, 'corrcoefBin_100E_Sbj10'), 'rb') as f:\n",
    "    all_corrcoef = pickle.load(f)\n",
    "fig_corrcoef = plt.figure(figsize=(15,10))\n",
    "plt.plot( all_corrcoef[0])\n",
    "plt.plot( all_corrcoef_val[0])\n",
    "plt.plot( all_corrcoef_test[0])\n",
    "plt.title('Correlation coefficient evaluation for 100 epoch when using binarization (-2..2)')\n",
    "plt.legend([\"Train corrcoef \", \"Validation corrcoef \", \"Test corrcoef\"], loc = 'best')\n",
    "fig_corrcoef.savefig(os.path.join(figure_path, 'Corrcoef '+str(len(all_corrcoef_test))+' epochs with binarization subj 10 finger 0.png'), format='png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ratios : \") \n",
    "print(ratios)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.utils.prune as prune\n",
    "\n",
    "# select a pruning technique from pytorch \n",
    "\n",
    "# choose the percentage of connections hat you would like to prune \n",
    "\n",
    "# it has do be pruned and then retrained on the remaining weights so the accuracy can go up again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pruning does work, more and more weight units are set to zero while running. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# todo : \n",
    "- complete table III report\n",
    "- do big run on all subjects for thrs 0.075\n",
    "- clean code \n",
    "- merge branch git hub \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# done : \n",
    "- try with multiple layers \n",
    "- find the right percentage of connections to prune \n",
    "- see if there is a dimension (channel) that is better for pruning than another \n",
    "- try to do our own class with a threshold instead of a percentage of \n",
    "- compute the number of pruned connection for each layer and globally \n",
    "- try with pruning on the linear layer \n",
    "- find the right lethod of pruning \n",
    "- find the right lethod of pruning "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "on veut enlever une colonne, donc correspond a un row de la matric de poids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
