{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trained Quantization & Weight Sharing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.io as sio \n",
    "import torch.nn as nn\n",
    "import torch \n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import argparse\n",
    "\n",
    "from preprocessing_funcs import get_spikes_with_history, preprocessing, remove_outliers\n",
    "from model import LSTM\n",
    "from trainer import train\n",
    "from quantizer import quantize_network, compute_quantized_weights, quantized_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set this to the root directory where you want to save and load data and figures\n",
    "root = os.path.join('drive')\n",
    "#set this to the diectory where you want to save data and checkpoints\n",
    "data_path = os.path.join(root, 'data')\n",
    "#set this to the diectory where you want to save checkpoints\n",
    "checkpoint_path = os.path.join(root, 'checkpoints')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Finger 0: 100%|########################################################################| 60/60 [09:33<00:00,  9.56s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run for subject 10 finger 4\n",
      "Correlation coefficient test : 0.3449362379260117\n"
     ]
    }
   ],
   "source": [
    "for Idx_subject in list([10]): # 3 subjects index 10-12\n",
    "\n",
    "       \n",
    "        for Finger in list([4]): # 5 fingers for each subject. 0:thumb, 1:index, 2:middle ...\n",
    "            \n",
    "            #load training data (TrainX: feature vectors, TrainY: labels)\n",
    "            matData = sio.loadmat(data_path + '/BCImoreData_Subj_'+str(Idx_subject)+'_200msLMP.mat')\n",
    "            TrainX = matData['Data_Feature'].transpose()\n",
    "            TrainY = matData['SmoothedFinger']\n",
    "            TrainY = TrainY [:,Finger]\n",
    "            TrainY = TrainY.reshape(TrainY.shape[0],1)\n",
    "            #load testing data (TestX: feature vectors, TestY: labels)\n",
    "            matData = sio.loadmat(data_path + '/BCImoreData_Subj_'+str(Idx_subject)+'_200msLMPTest.mat')\n",
    "            TestX = matData['Data_Feature'].transpose()\n",
    "            TestY = matData['SmoothedFinger']\n",
    "            TestY = TestY[:,Finger]\n",
    "            TestY = TestY.reshape(TestY.shape[0],1)\n",
    "            \n",
    "            # preprocessing \n",
    "            TrainX = remove_outliers(TrainX)\n",
    "            \n",
    "            x_scaler, y_scaler, TrainX, TestX, TrainY, TestY  = preprocessing(TrainX,TestX,TrainY,TestY)\n",
    "            \n",
    "            \n",
    "            # from here, we reconstruct the input by \"looking back\" a few steps\n",
    "            bins_before= 20 #How many bins of neural data prior to the output are used for decoding\n",
    "            bins_current=1 #Whether to use concurrent time bin of neural data\n",
    "            bins_after=0 #How many bins of neural data after the output are used for decoding\n",
    "            \n",
    "            TrainX=get_spikes_with_history(TrainX,bins_before,bins_after,bins_current)\n",
    "            TrainX, TrainY = TrainX[bins_before:,:,:], TrainY[bins_before:,]\n",
    "         \n",
    "            TestX=get_spikes_with_history(TestX,bins_before,bins_after,bins_current)\n",
    "            TestX, TestY = TestX[bins_before:,:,:], TestY[bins_before:,]\n",
    "            \n",
    "            # Now, we reconstructed TrainX/TestX to have a shape (num_of_samples, sequence_length, input_size)\n",
    "            # We can fit this to the LSTM\n",
    "            \n",
    "            print(\"Run for subject \" + str(Idx_subject) + \" finger \"+str(Finger))\n",
    "\n",
    "            n_hidden = 20\n",
    "            n_layers = 5\n",
    "            input_dim = TrainX.shape[2]\n",
    "            output_dim = TrainY.shape[1]\n",
    "            seq_len =  TrainX.shape[1]\n",
    "\n",
    "            net = LSTM(input_dim, output_dim, seq_len,  n_hidden, n_layers)\n",
    "\n",
    "            lossfunc = nn.MSELoss()\n",
    "\n",
    "            optimizer = torch.optim.Adamax(net.parameters(), lr=0.002)\n",
    "            net.train()\n",
    "\n",
    "\n",
    "            ##training the initial model\n",
    "            PATH_pre_trained = checkpoint_path + '/s'+ str(Idx_subject) + '_f'+str(Finger)+'_trained_model'\n",
    "            \n",
    "            net.load_state_dict(torch.load(PATH_pre_trained))\n",
    "\n",
    "            ##test initial model\n",
    "            net.eval()\n",
    "            pred,h = net(torch.from_numpy(TestX).float(), net.init_hidden(TestX.shape[0]))\n",
    "            pred = pred.detach().numpy()[-1,:,:]\n",
    "            pred = y_scaler.inverse_transform(pred)\n",
    "            TestY = y_scaler.inverse_transform(TestY)\n",
    "            pred = pred.reshape((-1,))\n",
    "            corrcoef = np.corrcoef(pred,TestY.reshape((-1,)))\n",
    "\n",
    "            \n",
    "            print ('Correlation coefficient test : {corrcoef}'.format(corrcoef=corrcoef[0,1]))   \n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quantited Model Before Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correlation coefficient test : 0.338057174990737\n"
     ]
    }
   ],
   "source": [
    "#initialize the quantiezed weights using the weights from the trained netwrok:\n",
    "k=8\n",
    "net = compute_quantized_weights(net,k)\n",
    "#set the model's parameters to their quantized version\n",
    "net = quantize_network(net)\n",
    "net.eval()\n",
    "quant_pred,h = net(torch.from_numpy(TestX).float(), net.init_hidden(TestX.shape[0]), quant=True)\n",
    "quant_pred = quant_pred.detach().numpy()[-1,:,:]\n",
    "quant_pred = y_scaler.inverse_transform(quant_pred)\n",
    "TestY = y_scaler.inverse_transform(TestY)\n",
    "quant_pred = quant_pred.reshape((-1,))\n",
    "quant_corrcoef = np.corrcoef(quant_pred,TestY.reshape((-1,)))\n",
    "\n",
    "print ('Correlation coefficient test : {corrcoef}'.format(corrcoef=quant_corrcoef[0,1]))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quantited Model After Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Finger 0:   0%|                                                                                 | 0/30 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuning Centroids...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Finger 0: 100%|########################################################################| 30/30 [03:59<00:00,  7.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correlation coefficient test : 0.36211613091441164\n"
     ]
    }
   ],
   "source": [
    "#re-loas the weights:\n",
    "net.load_state_dict(torch.load(PATH_pre_trained))\n",
    "k=8\n",
    "#initialize the quantiezed weights using the weights from the trained netwrok:\n",
    "net = compute_quantized_weights(net,k)\n",
    "net.train()\n",
    "\n",
    "#train the quantized netwok\n",
    "quantized_corr_train, quantized_corr_val, quantized_corr_test = quantized_train(TrainX, TrainY,TestX,TestY, net, lossfunc, optimizer, num_epoch = 30, clip = 5)\n",
    "#set the model's parameters to their quantized version\n",
    "net = quantize_network(net)\n",
    "\n",
    "\n",
    "net.eval()\n",
    "quant_pred,h = net(torch.from_numpy(TestX).float(), net.init_hidden(TestX.shape[0]), quant=True)\n",
    "quant_pred = quant_pred.detach().numpy()[-1,:,:]\n",
    "quant_pred = y_scaler.inverse_transform(quant_pred)\n",
    "TestY = y_scaler.inverse_transform(TestY)\n",
    "quant_pred = quant_pred.reshape((-1,))\n",
    "quant_corrcoef = np.corrcoef(quant_pred,TestY.reshape((-1,)))\n",
    "\n",
    "\n",
    "print ('Correlation coefficient test : {corrcoef}'.format(corrcoef=quant_corrcoef[0,1])) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cluster Distibution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# matplotlib histogram\n",
    "indices = net.quantized_state_dict['lstm.weight_ih_l4'][0].numpy()\n",
    "plt.hist(indices, color = 'blue', edgecolor = 'black',\n",
    "        bins = int(180/5))\n",
    "\n",
    "plt.title('Subject_'+str(Idx_subject) + '_Finger_' + str(Finger))\n",
    "plt.xlabel('Cluster Index')\n",
    "plt.ylabel('Count')\n",
    "\n",
    "plt.savefig('Subject_'+str(Idx_subject) + 'Finger_' + str(Finger))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
